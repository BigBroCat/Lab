{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsA3fHs8xhTrEs9mtg9S5K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BigBroCat/Lab/blob/main/Monte_Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wmPQS9MAN5Z",
        "outputId": "569c695d-f114-4f48-9c02-6c124e5cbd04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Estimated Value Function:\n",
            "A: 0.0728\n",
            "B: 0.2758\n",
            "C: 0.5192\n",
            "D: 0.6758\n",
            "E: 0.8497\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "gamma = 0.9  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "tolerance = 1e-3  # Convergence tolerance\n",
        "max_episodes = 1000  # Maximum number of episodes\n",
        "states = [\"A\", \"B\", \"C\", \"D\", \"E\"]  # Non-terminal states\n",
        "terminal_states = [\"Terminal (Left)\", \"Terminal (Right)\"]\n",
        "all_states = terminal_states[:1] + states + terminal_states[1:]\n",
        "true_values = [0.0, 1/6, 2/6, 3/6, 4/6, 5/6, 1.0]  # Ground-truth values for RMS calculation\n",
        "\n",
        "# Initialize value function\n",
        "V = {state: 0.0 for state in all_states}\n",
        "\n",
        "# Random policy: Transition probabilities are equal\n",
        "def random_walk(state):\n",
        "    \"\"\"Simulates a random walk from a given state.\"\"\"\n",
        "    if state in terminal_states:\n",
        "        return state, 0  # Terminal state, no reward\n",
        "\n",
        "    idx = all_states.index(state)\n",
        "    next_state = np.random.choice([all_states[idx - 1], all_states[idx + 1]])\n",
        "    reward = 0 if next_state not in terminal_states else (1 if next_state == \"Terminal (Right)\" else 0)\n",
        "    return next_state, reward\n",
        "\n",
        "# Monte Carlo Every-Visit Learning Algorithm\n",
        "def monte_carlo_learning(alpha, gamma, max_episodes, tolerance):\n",
        "    \"\"\"Monte Carlo Every-Visit Learning for Value Function Estimation.\"\"\"\n",
        "    global V\n",
        "    rms_errors = []  # Track RMS error for each episode\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        # Generate an episode\n",
        "        episode_data = []  # Stores (state, reward) pairs\n",
        "        current_state = \"C\"  # Start from the middle state\n",
        "        while current_state not in terminal_states:\n",
        "            next_state, reward = random_walk(current_state)\n",
        "            episode_data.append((current_state, reward))\n",
        "            current_state = next_state\n",
        "        episode_data.append((current_state, 0))  # Add terminal state\n",
        "\n",
        "        # Compute returns G_t in reverse order and update V\n",
        "        G = 0\n",
        "        visited = set()  # To ensure every-visit\n",
        "        for state, reward in reversed(episode_data):\n",
        "            G = reward + gamma * G\n",
        "            if state not in visited and state not in terminal_states:\n",
        "                visited.add(state)\n",
        "                V[state] += alpha * (G - V[state])  # Every-visit update\n",
        "\n",
        "        # Compute RMS error for this episode\n",
        "        rms_error = compute_rms_error(V, true_values)\n",
        "        rms_errors.append(rms_error)\n",
        "\n",
        "        # Convergence check: Stop if RMS error change is small\n",
        "        if len(rms_errors) > 1 and abs(rms_errors[-1] - rms_errors[-2]) < tolerance:\n",
        "            break\n",
        "\n",
        "    return V, rms_errors\n",
        "\n",
        "# Compute RMS Error\n",
        "def compute_rms_error(V, true_values):\n",
        "    \"\"\"Calculate RMS error compared to true state values.\"\"\"\n",
        "    errors = [\n",
        "        (V[states[i]] - true_values[i + 1]) ** 2  # Skip terminal states\n",
        "        for i in range(len(states))\n",
        "    ]\n",
        "    return np.sqrt(np.mean(errors))\n",
        "\n",
        "# Run Monte Carlo Learning\n",
        "V, rms_errors = monte_carlo_learning(alpha, gamma, max_episodes, tolerance)\n",
        "\n",
        "# Display Final Value Function\n",
        "print(\"Final Estimated Value Function:\")\n",
        "for state in states:\n",
        "    print(f\"{state}: {V[state]:.4f}\")\n",
        "\n"
      ]
    }
  ]
}